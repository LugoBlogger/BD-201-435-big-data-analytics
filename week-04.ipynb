{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week we want to analyze the vocabulary in Pride and Prejudice novel\n",
    "by Jane Austen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 49406)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ASUS\\miniconda3\\envs\\learnPySpark\\Lib\\socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"c:\\Users\\ASUS\\miniconda3\\envs\\learnPySpark\\Lib\\socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"c:\\Users\\ASUS\\miniconda3\\envs\\learnPySpark\\Lib\\socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"c:\\Users\\ASUS\\miniconda3\\envs\\learnPySpark\\Lib\\socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"c:\\Users\\ASUS\\miniconda3\\envs\\learnPySpark\\Lib\\site-packages\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"c:\\Users\\ASUS\\miniconda3\\envs\\learnPySpark\\Lib\\site-packages\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"c:\\Users\\ASUS\\miniconda3\\envs\\learnPySpark\\Lib\\site-packages\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ASUS\\miniconda3\\envs\\learnPySpark\\Lib\\site-packages\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ASUS\\miniconda3\\envs\\learnPySpark\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "  .builder\n",
    "  .master(\"local[*]\") \n",
    "  .appName(\"Analyzing the vocabulary of Pride and Prejudice.\")\n",
    "  .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-2P5TQC27:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Analyzing the vocabulary of Pride and Prejudice.</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2adef103f10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full program will be divided into five subprogram\n",
    "1. \n",
    "2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book = spark.read.text(\n",
    "  \"./data/gutenberg_books/1342-0.txt\")\n",
    "book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some rows from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|The Project Guten...|\n",
      "|                    |\n",
      "|This eBook is for...|\n",
      "|almost no restric...|\n",
      "|re-use it under t...|\n",
      "|with this eBook o...|\n",
      "|                    |\n",
      "|                    |\n",
      "|Title: Pride and ...|\n",
      "|                    |\n",
      "| Author: Jane Austen|\n",
      "|                    |\n",
      "|Posting Date: Aug...|\n",
      "|Release Date: Jun...|\n",
      "|Last Updated: Mar...|\n",
      "|                    |\n",
      "|   Language: English|\n",
      "|                    |\n",
      "|Character set enc...|\n",
      "|                    |\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|                                             value|\n",
      "+--------------------------------------------------+\n",
      "|The Project Gutenberg EBook of Pride and Prejud...|\n",
      "|                                                  |\n",
      "|This eBook is for the use of anyone anywhere at...|\n",
      "|almost no restrictions whatsoever.  You may cop...|\n",
      "|re-use it under the terms of the Project Gutenb...|\n",
      "|    with this eBook or online at www.gutenberg.org|\n",
      "|                                                  |\n",
      "|                                                  |\n",
      "|                        Title: Pride and Prejudice|\n",
      "|                                                  |\n",
      "+--------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book.show(10, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[line: array<string>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = book.select(\n",
    "  F.split(book.value, \" \")\n",
    "   .alias(\"line\"))\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- line: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|                                              line|\n",
      "+--------------------------------------------------+\n",
      "|[The, Project, Gutenberg, EBook, of, Pride, and...|\n",
      "|                                                []|\n",
      "|[This, eBook, is, for, the, use, of, anyone, an...|\n",
      "|[almost, no, restrictions, whatsoever., , You, ...|\n",
      "|[re-use, it, under, the, terms, of, the, Projec...|\n",
      "+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a column `word`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = lines.select(\n",
    "  F.explode(F.col(\"line\"))\n",
    "   .alias(\"word\"))\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|       The|\n",
      "|   Project|\n",
      "| Gutenberg|\n",
      "|     EBook|\n",
      "|        of|\n",
      "|     Pride|\n",
      "|       and|\n",
      "|Prejudice,|\n",
      "|        by|\n",
      "|      Jane|\n",
      "|    Austen|\n",
      "|          |\n",
      "|      This|\n",
      "|     eBook|\n",
      "|        is|\n",
      "+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words.show(15, truncate=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word_lower: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_lower = words.select(\n",
    "  F.lower(F.col(\"word\")).alias(\"word_lower\"))\n",
    "words_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word_lower: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_lower.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|word_lower|\n",
      "+----------+\n",
      "|       the|\n",
      "|   project|\n",
      "| gutenberg|\n",
      "|     ebook|\n",
      "|        of|\n",
      "|     pride|\n",
      "|       and|\n",
      "|prejudice,|\n",
      "|        by|\n",
      "|      jane|\n",
      "|    austen|\n",
      "|          |\n",
      "|      this|\n",
      "|     ebook|\n",
      "|        is|\n",
      "|       for|\n",
      "|       the|\n",
      "|       use|\n",
      "|        of|\n",
      "|    anyone|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_lower.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation (comma, period, apostrophe, full colon, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|    pride|\n",
      "|      and|\n",
      "|prejudice|\n",
      "|       by|\n",
      "|     jane|\n",
      "|   austen|\n",
      "|         |\n",
      "|     this|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_clean = words_lower.select(\n",
    "  F.regexp_extract(F.col(\"word_lower\"), \n",
    "                   \"[a-z]+\", 0)\n",
    "   .alias(\"word\"))\n",
    "words_clean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the third argument in `regexp_extract`, see the following\n",
    "example. This example is from the Python documentation about\n",
    "`group()` in Python's regex.   \n",
    "https://docs.python.org/3/library/re.html#re.Match.group\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isaac Newton\n",
      "Isaac\n",
      "Newton\n",
      "('Isaac', 'Newton')\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "m = re.match(r\"(\\w+) (\\w+)\", \"Isaac Newton\")\n",
    "print(m.group(0))       # The entire match\n",
    "print(m.group(1))       # The first parenthesized subgroup\n",
    "print(m.group(2))       # The second parenthesized subgroup\n",
    "print(m.group(1, 2))    # Multiple arguments give us a tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples from the documentation of `pyspark.sql.functions`.\n",
    "See https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regexp_extract.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(d='100-200')]\n",
      "[Row(d='100')]\n",
      "[Row(d='200')]\n"
     ]
    }
   ],
   "source": [
    "df_test = spark.createDataFrame([(\"100-200\", )], [\"str\"])\n",
    "print(df_test.select(F.regexp_extract(\"str\", \"(\\d+)-(\\d+)\", 0).alias('d')).collect())\n",
    "print(df_test.select(F.regexp_extract(\"str\", \"(\\d+)-(\\d+)\", 1).alias('d')).collect())\n",
    "print(df_test.select(F.regexp_extract(\"str\", \"(\\d+)-(\\d+)\", 2).alias('d')).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to the `word_clean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     word|\n",
      "+---------+\n",
      "|      the|\n",
      "|  project|\n",
      "|gutenberg|\n",
      "|    ebook|\n",
      "|       of|\n",
      "|    pride|\n",
      "|      and|\n",
      "|prejudice|\n",
      "|       by|\n",
      "|     jane|\n",
      "|   austen|\n",
      "|     this|\n",
      "|    ebook|\n",
      "|       is|\n",
      "|      for|\n",
      "|      the|\n",
      "|      use|\n",
      "|       of|\n",
      "|   anyone|\n",
      "| anywhere|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_nonull = words_clean.filter(\n",
    "    F.col(\"word\") != \"\")\n",
    "words_nonull.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Presenting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnPySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
