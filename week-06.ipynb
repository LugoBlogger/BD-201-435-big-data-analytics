{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run `word_count_submit.py` with  \n",
    "```bash\n",
    "pyspark-submit word_count_submit.py\n",
    "```\n",
    "in Anaconda Prompt (miniconda3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week, we want to compare how fast is pyspark to process eight books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "  .builder\n",
    "  .master(\"local[*]\")   # optional\n",
    "  .appName(\"Word Counts Program.\")\n",
    "  .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single novel (Jane Austen 1813 - Pride and Prejudice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the| 4496|\n",
      "|  to| 4235|\n",
      "|  of| 3719|\n",
      "| and| 3602|\n",
      "| her| 2223|\n",
      "|   i| 2052|\n",
      "|   a| 1997|\n",
      "|  in| 1920|\n",
      "| was| 1844|\n",
      "| she| 1703|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Computational times: (10.3 secs first run, 0.2 - 0.4 secs second runs)\n",
    "results = (\n",
    "  spark.read.text(\"./data/gutenberg_books/1342-0.txt\")\n",
    "  .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "  .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "  .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "  .select(F.regexp_extract(F.col(\"word\"), \"[a-z]+\", 0).alias(\"word\"))\n",
    "  .where(F.col(\"word\") != \"\")\n",
    "  .groupby(\"word\")\n",
    "  .count()\n",
    ")\n",
    "\n",
    "# Show the top 10 of the most occurrence words in Jane Austen - Pride and Prejudice\n",
    "results.orderBy(\"count\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 classical books\n",
    "- `11-0.txt`: Lewis Carol (1865) - Alice's Adventures in Wonderland\n",
    "- `84-0.txt`: Mary Shelley (1818) - Frankenstein; or, The Modern Promotheus\n",
    "- `1342-0.txt`: Jane Austen (1813) - Pride and Prejudice\n",
    "- `1661-0.txt`: Arthur Conan Doyle (1892) - The Adventures of Sherlock Holmes\n",
    "- `2701-0.txt`: Herman Melville (1851) - Moby-Dick; or, The Whale\n",
    "- `pg132.txt`: 孫子/Sun Tzu (5th century BC) - 孫子兵法 (The Art of War / Sun Tzu's Military Method) \n",
    "- `pg514.txt`: Louisa May Alcott (1868-1869) - Little Women\n",
    "- `pg1399.txt`: Лев Толстой/Leo Tolstoy (1878) - Анна Каренина (Anna Karenina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the|60331|\n",
      "| and|39571|\n",
      "|  to|31793|\n",
      "|  of|30994|\n",
      "|   a|23322|\n",
      "|  in|19247|\n",
      "|   i|19189|\n",
      "|that|15784|\n",
      "|  he|15253|\n",
      "|  it|14744|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Computational time: 5.1 secs for first run; 0.5 - 0.7 for the second runs\n",
    "results = (\n",
    "  spark.read.text(\"./data/gutenberg_books/*.txt\")\n",
    "  .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "  .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "  .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "  .select(F.regexp_extract(F.col(\"word\"), \"[a-z]+\", 0).alias(\"word\"))\n",
    "  .where(F.col(\"word\") != \"\")\n",
    "  .groupby(\"word\")\n",
    "  .count()\n",
    ")\n",
    "\n",
    "# Show the top 10 of the most occurrence words in all books inside `data/project-gutenberg/`\n",
    "results.orderBy(\"count\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare to `pandas` implementation.   \n",
    "It takes so long to calculate compare to when using `pyspark`.   \n",
    "`pandas` uses __eager evaluation__ and  \n",
    "`pyspark` uses __lazy evaluation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>the</td>\n",
       "      <td>60331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>and</td>\n",
       "      <td>39571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>to</td>\n",
       "      <td>31793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>of</td>\n",
       "      <td>30994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>a</td>\n",
       "      <td>23322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in</td>\n",
       "      <td>19247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>i</td>\n",
       "      <td>19189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>that</td>\n",
       "      <td>15784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>he</td>\n",
       "      <td>15253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>it</td>\n",
       "      <td>14744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  count\n",
       "13    the  60331\n",
       "21    and  39571\n",
       "66     to  31793\n",
       "15     of  30994\n",
       "90      a  23322\n",
       "4      in  19247\n",
       "61      i  19189\n",
       "127  that  15784\n",
       "686    he  15253\n",
       "29     it  14744"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob(\"./data/gutenberg_books/*.txt\")\n",
    "\n",
    "lines = []\n",
    "for file in files:\n",
    "  with open(file, 'r') as f:\n",
    "    lines += f.readlines()\n",
    "\n",
    "results = pd.DataFrame(lines, columns=[\"value\"])\n",
    "results[\"value\"] = results[\"value\"].str.split(\" \")\n",
    "results = results.explode(\"value\")\n",
    "results.rename(columns={\"value\": \"word\"}, inplace=True)\n",
    "results[\"word\"] = results[\"word\"].str.lower()\n",
    "results[\"word\"] = results[\"word\"].str.extract(r\"([a-z]+)\")\n",
    "\n",
    "# results.head(30)\n",
    "# results[results[\"word\"].isna() == True]\n",
    "results.dropna(inplace=True)\n",
    "# results.reset_index(inplace=True, drop=True)\n",
    "results[\"count\"] = 1\n",
    "results = results.groupby(by=[\"word\"], as_index=False, sort=False).count()\n",
    "results.sort_values(by=[\"count\"], inplace=True, ascending=False)\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program Broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset from the Google Drive with you ITK's account:\n",
    "https://drive.google.com/drive/u/0/folders/1-PndwIh7saR0ZocE3Ujslx8vei-HU9k8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "  .builder\n",
    "  .master(\"local[*]\")   # optional\n",
    "  .appName(\"Processing Tabular Data\")\n",
    "  .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read broadcast data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIRECTORY = \"./data/broadcast_logs\"   # use this when you present to the student\n",
    "DIRECTORY = \"../rioux-2022/data/broadcast_logs\"\n",
    "\n",
    "broadcast_logs_filename = \"BroadcastLogs_2018_Q3_M8_sample.CSV\"\n",
    "# broadcast_logs_filename = \"BroadcastLogs_2018_Q3_M8.CSV\"\n",
    "\n",
    "logs = spark.read.csv(\n",
    "  os.path.join(DIRECTORY, broadcast_logs_filename),\n",
    "  sep=\"|\",\n",
    "  header=True,\n",
    "  inferSchema=True,\n",
    "  timestampFormat=\"yyyy-MM-dd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BroadcastLogID: integer (nullable = true)\n",
      " |-- LogServiceID: integer (nullable = true)\n",
      " |-- LogDate: date (nullable = true)\n",
      " |-- SequenceNO: integer (nullable = true)\n",
      " |-- AudienceTargetAgeID: integer (nullable = true)\n",
      " |-- AudienceTargetEthnicID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- ClosedCaptionID: integer (nullable = true)\n",
      " |-- CountryOfOriginID: integer (nullable = true)\n",
      " |-- DubDramaCreditID: integer (nullable = true)\n",
      " |-- EthnicProgramID: integer (nullable = true)\n",
      " |-- ProductionSourceID: integer (nullable = true)\n",
      " |-- ProgramClassID: integer (nullable = true)\n",
      " |-- FilmClassificationID: integer (nullable = true)\n",
      " |-- ExhibitionID: integer (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- EndTime: string (nullable = true)\n",
      " |-- LogEntryDate: date (nullable = true)\n",
      " |-- ProductionNO: string (nullable = true)\n",
      " |-- ProgramTitle: string (nullable = true)\n",
      " |-- StartTime: string (nullable = true)\n",
      " |-- Subtitle: string (nullable = true)\n",
      " |-- NetworkAffiliationID: integer (nullable = true)\n",
      " |-- SpecialAttentionID: integer (nullable = true)\n",
      " |-- BroadcastOriginPointID: integer (nullable = true)\n",
      " |-- CompositionID: integer (nullable = true)\n",
      " |-- Producer1: string (nullable = true)\n",
      " |-- Producer2: string (nullable = true)\n",
      " |-- Language1: integer (nullable = true)\n",
      " |-- Language2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+----------+----------+-------------------+----------------------+----------+---------------+-----------------+----------------+---------------+------------------+--------------+--------------------+------------+----------------+----------------+------------+------------+--------------------+----------------+--------+--------------------+------------------+----------------------+-------------+---------+---------+---------+---------+\n",
      "|BroadcastLogID|LogServiceID|   LogDate|SequenceNO|AudienceTargetAgeID|AudienceTargetEthnicID|CategoryID|ClosedCaptionID|CountryOfOriginID|DubDramaCreditID|EthnicProgramID|ProductionSourceID|ProgramClassID|FilmClassificationID|ExhibitionID|        Duration|         EndTime|LogEntryDate|ProductionNO|        ProgramTitle|       StartTime|Subtitle|NetworkAffiliationID|SpecialAttentionID|BroadcastOriginPointID|CompositionID|Producer1|Producer2|Language1|Language2|\n",
      "+--------------+------------+----------+----------+-------------------+----------------------+----------+---------------+-----------------+----------------+---------------+------------------+--------------+--------------------+------------+----------------+----------------+------------+------------+--------------------+----------------+--------+--------------------+------------------+----------------------+-------------+---------+---------+---------+---------+\n",
      "|    1196192316|        3157|2018-08-01|         1|                  4|                  NULL|        13|              3|                3|            NULL|           NULL|                10|            19|                NULL|           2|02:00:00.0000000|08:00:00.0000000|  2018-08-01|      A39082|   Newlywed and Dead|06:00:00.0000000|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|       94|     NULL|\n",
      "|    1196192317|        3157|2018-08-01|         2|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|            20|                NULL|        NULL|00:00:30.0000000|06:13:45.0000000|  2018-08-01|        NULL|15-SPECIALTY CHAN...|06:13:15.0000000|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|\n",
      "|    1196192318|        3157|2018-08-01|         3|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|00:00:15.0000000|06:14:00.0000000|  2018-08-01|        NULL|3-PROCTER & GAMBL...|06:13:45.0000000|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|\n",
      "|    1196192319|        3157|2018-08-01|         4|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|00:00:15.0000000|06:14:15.0000000|  2018-08-01|        NULL|12-CREDIT KARMA-B...|06:14:00.0000000|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|\n",
      "|    1196192320|        3157|2018-08-01|         5|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|00:00:15.0000000|06:14:30.0000000|  2018-08-01|        NULL|3-L'OREAL CANADA-...|06:14:15.0000000|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|\n",
      "|    1196192321|        3157|2018-08-01|         6|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|00:00:15.0000000|06:14:45.0000000|  2018-08-01|        NULL|11-YUM! BRANDS-Ch...|06:14:30.0000000|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|\n",
      "|    1196192322|        3157|2018-08-01|         7|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|00:00:30.0000000|06:15:16.0000000|  2018-08-01|        NULL|2-PIER 1 IMPORTS ...|06:14:46.0000000|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|\n",
      "|    1196192323|        3157|2018-08-01|         8|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|00:00:15.0000000|06:15:31.0000000|  2018-08-01|        NULL|3-HAVAS EDGE-Trav...|06:15:16.0000000|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|\n",
      "|    1196192324|        3157|2018-08-01|         9|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|00:00:15.0000000|06:15:46.0000000|  2018-08-01|        NULL|2-AUTOTRADER-Inte...|06:15:31.0000000|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|\n",
      "|    1196192325|        3157|2018-08-01|        10|               NULL|                  NULL|      NULL|              1|             NULL|            NULL|           NULL|              NULL|             3|                NULL|        NULL|00:00:15.0000000|06:16:01.0000000|  2018-08-01|        NULL|11-SLEEP COUNTRY ...|06:15:46.0000000|    NULL|                NULL|              NULL|                  NULL|         NULL|     NULL|     NULL|     NULL|     NULL|\n",
      "+--------------+------------+----------+----------+-------------------+----------------------+----------+---------------+-----------------+----------------+---------------+------------------+--------------+--------------------+------------+----------------+----------------+------------+------------+--------------------+----------------+--------+--------------------+------------------+----------------------+-------------+---------+---------+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing all columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BroadcastLogID',\n",
       " 'LogServiceID',\n",
       " 'LogDate',\n",
       " 'SequenceNO',\n",
       " 'AudienceTargetAgeID',\n",
       " 'AudienceTargetEthnicID',\n",
       " 'CategoryID',\n",
       " 'ClosedCaptionID',\n",
       " 'CountryOfOriginID',\n",
       " 'DubDramaCreditID',\n",
       " 'EthnicProgramID',\n",
       " 'ProductionSourceID',\n",
       " 'ProgramClassID',\n",
       " 'FilmClassificationID',\n",
       " 'ExhibitionID',\n",
       " 'Duration',\n",
       " 'EndTime',\n",
       " 'LogEntryDate',\n",
       " 'ProductionNO',\n",
       " 'ProgramTitle',\n",
       " 'StartTime',\n",
       " 'Subtitle',\n",
       " 'NetworkAffiliationID',\n",
       " 'SpecialAttentionID',\n",
       " 'BroadcastOriginPointID',\n",
       " 'CompositionID',\n",
       " 'Producer1',\n",
       " 'Producer2',\n",
       " 'Language1',\n",
       " 'Language2']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the table in a group of three columns.    \n",
    "First we split array column into three items on each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['BroadcastLogID', 'LogServiceID', 'LogDate'], dtype='<U22'), array(['SequenceNO', 'AudienceTargetAgeID', 'AudienceTargetEthnicID'],\n",
      "      dtype='<U22'), array(['CategoryID', 'ClosedCaptionID', 'CountryOfOriginID'], dtype='<U22'), array(['DubDramaCreditID', 'EthnicProgramID', 'ProductionSourceID'],\n",
      "      dtype='<U22'), array(['ProgramClassID', 'FilmClassificationID', 'ExhibitionID'],\n",
      "      dtype='<U22'), array(['Duration', 'EndTime', 'LogEntryDate'], dtype='<U22'), array(['ProductionNO', 'ProgramTitle', 'StartTime'], dtype='<U22'), array(['Subtitle', 'NetworkAffiliationID', 'SpecialAttentionID'],\n",
      "      dtype='<U22'), array(['BroadcastOriginPointID', 'CompositionID', 'Producer1'],\n",
      "      dtype='<U22'), array(['Producer2', 'Language1', 'Language2'], dtype='<U22')]\n"
     ]
    }
   ],
   "source": [
    "column_split = np.array_split(\n",
    "  np.array(logs.columns), len(logs.columns) // 3)\n",
    "print(column_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+----------+\n",
      "|BroadcastLogID|LogServiceID|LogDate   |\n",
      "+--------------+------------+----------+\n",
      "|1196192316    |3157        |2018-08-01|\n",
      "|1196192317    |3157        |2018-08-01|\n",
      "|1196192318    |3157        |2018-08-01|\n",
      "|1196192319    |3157        |2018-08-01|\n",
      "|1196192320    |3157        |2018-08-01|\n",
      "+--------------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-------------------+----------------------+\n",
      "|SequenceNO|AudienceTargetAgeID|AudienceTargetEthnicID|\n",
      "+----------+-------------------+----------------------+\n",
      "|1         |4                  |NULL                  |\n",
      "|2         |NULL               |NULL                  |\n",
      "|3         |NULL               |NULL                  |\n",
      "|4         |NULL               |NULL                  |\n",
      "|5         |NULL               |NULL                  |\n",
      "+----------+-------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+---------------+-----------------+\n",
      "|CategoryID|ClosedCaptionID|CountryOfOriginID|\n",
      "+----------+---------------+-----------------+\n",
      "|13        |3              |3                |\n",
      "|NULL      |1              |NULL             |\n",
      "|NULL      |1              |NULL             |\n",
      "|NULL      |1              |NULL             |\n",
      "|NULL      |1              |NULL             |\n",
      "+----------+---------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+---------------+------------------+\n",
      "|DubDramaCreditID|EthnicProgramID|ProductionSourceID|\n",
      "+----------------+---------------+------------------+\n",
      "|NULL            |NULL           |10                |\n",
      "|NULL            |NULL           |NULL              |\n",
      "|NULL            |NULL           |NULL              |\n",
      "|NULL            |NULL           |NULL              |\n",
      "|NULL            |NULL           |NULL              |\n",
      "+----------------+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+--------------------+------------+\n",
      "|ProgramClassID|FilmClassificationID|ExhibitionID|\n",
      "+--------------+--------------------+------------+\n",
      "|19            |NULL                |2           |\n",
      "|20            |NULL                |NULL        |\n",
      "|3             |NULL                |NULL        |\n",
      "|3             |NULL                |NULL        |\n",
      "|3             |NULL                |NULL        |\n",
      "+--------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------+----------------+------------+\n",
      "|Duration        |EndTime         |LogEntryDate|\n",
      "+----------------+----------------+------------+\n",
      "|02:00:00.0000000|08:00:00.0000000|2018-08-01  |\n",
      "|00:00:30.0000000|06:13:45.0000000|2018-08-01  |\n",
      "|00:00:15.0000000|06:14:00.0000000|2018-08-01  |\n",
      "|00:00:15.0000000|06:14:15.0000000|2018-08-01  |\n",
      "|00:00:15.0000000|06:14:30.0000000|2018-08-01  |\n",
      "+----------------+----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+-------------------------------------------+----------------+\n",
      "|ProductionNO|ProgramTitle                               |StartTime       |\n",
      "+------------+-------------------------------------------+----------------+\n",
      "|A39082      |Newlywed and Dead                          |06:00:00.0000000|\n",
      "|NULL        |15-SPECIALTY CHANNELS-Canadian Generic     |06:13:15.0000000|\n",
      "|NULL        |3-PROCTER & GAMBLE INC-Anti-Perspirant 3rd |06:13:45.0000000|\n",
      "|NULL        |12-CREDIT KARMA-Bank/Credit Union/Trust 3rd|06:14:00.0000000|\n",
      "|NULL        |3-L'OREAL CANADA-Hair Products 3rd         |06:14:15.0000000|\n",
      "+------------+-------------------------------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+--------------------+------------------+\n",
      "|Subtitle|NetworkAffiliationID|SpecialAttentionID|\n",
      "+--------+--------------------+------------------+\n",
      "|NULL    |NULL                |NULL              |\n",
      "|NULL    |NULL                |NULL              |\n",
      "|NULL    |NULL                |NULL              |\n",
      "|NULL    |NULL                |NULL              |\n",
      "|NULL    |NULL                |NULL              |\n",
      "+--------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------------+-------------+---------+\n",
      "|BroadcastOriginPointID|CompositionID|Producer1|\n",
      "+----------------------+-------------+---------+\n",
      "|NULL                  |NULL         |NULL     |\n",
      "|NULL                  |NULL         |NULL     |\n",
      "|NULL                  |NULL         |NULL     |\n",
      "|NULL                  |NULL         |NULL     |\n",
      "|NULL                  |NULL         |NULL     |\n",
      "+----------------------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+---------+---------+\n",
      "|Producer2|Language1|Language2|\n",
      "+---------+---------+---------+\n",
      "|NULL     |94       |NULL     |\n",
      "|NULL     |NULL     |NULL     |\n",
      "|NULL     |NULL     |NULL     |\n",
      "|NULL     |NULL     |NULL     |\n",
      "|NULL     |NULL     |NULL     |\n",
      "+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in column_split:\n",
    "  logs.select(*x).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove column using `.drop()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "logs_clean = logs.drop(\"BroadcastLogID\", \"SequenceNO\")\n",
    "\n",
    "# Testing if we effectively got rid of the columns\n",
    "print(\"BroadcastLogID\" in logs_clean.columns)\n",
    "print(\"SequenceNo\" in logs_clean.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View `Duration` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|        Duration|\n",
      "+----------------+\n",
      "|02:00:00.0000000|\n",
      "|00:00:30.0000000|\n",
      "|00:00:15.0000000|\n",
      "|00:00:15.0000000|\n",
      "|00:00:15.0000000|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[('Duration', 'string')]\n"
     ]
    }
   ],
   "source": [
    "logs_clean.select(F.col(\"Duration\")).show(5)\n",
    "\n",
    "print(logs_clean.select(F.col(\"Duration\")).dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse hour, minute and second for each row in `logs_clean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+-----------+-----------+\n",
      "|        Duration|dur_hours|dur_minutes|dur_seconds|\n",
      "+----------------+---------+-----------+-----------+\n",
      "|00:04:52.0000000|        0|          4|         52|\n",
      "|00:10:06.0000000|        0|         10|          6|\n",
      "|00:09:52.0000000|        0|          9|         52|\n",
      "|00:04:26.0000000|        0|          4|         26|\n",
      "|00:14:59.0000000|        0|         14|         59|\n",
      "+----------------+---------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_clean.select(\n",
    "  F.col(\"Duration\"),\n",
    "  F.col(\"Duration\").substr(1, 2).cast(\"int\").alias(\"dur_hours\"),\n",
    "  F.col(\"Duration\").substr(4, 2).cast(\"int\").alias(\"dur_minutes\"),\n",
    "  F.col(\"Duration\").substr(7, 2).cast(\"int\").alias(\"dur_seconds\"),\n",
    ").distinct().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will discuss next week for getting the above time parsing columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnPySpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
